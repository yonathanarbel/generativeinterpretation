{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3aaf936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Multi-Source Embedding Analysis\\n\\nThis code supports the Katrina case analysis provided in Arbel & Hoffman, Generative Interpretation\\n\\nNote, the model is best run in a high-ram enviornment, possibly using a GPU. We used 250 GB of RAM, thanks to the University of Alabama High Power Compute Center\\n\\nThe scripts creates embeddings for a context sentence and a number of reference sentences, measure relative distances, normalizes them between 0-1, and presents results.\\nWe are using the top 10 embedding models on the HuggingFace MTEB Leaderboard for STS tasks as they were on 10.1.23\\n(The leading model--Sionic v2--doesn't currently have an API)\\n\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Multi-Source Embedding Analysis\n",
    "\n",
    "This code supports the Katrina case analysis provided in Arbel & Hoffman, Generative Interpretation\n",
    "\n",
    "Note, the model is best run in a high-ram enviornment, possibly using a GPU. We used 250 GB of RAM, thanks to the University of Alabama High Power Compute Center\n",
    "\n",
    "The scripts creates embeddings for a context sentence and a number of reference sentences, measure relative distances, normalizes them between 0-1, and presents results.\n",
    "We are using the top 10 embedding models on the HuggingFace MTEB Leaderboard for STS tasks as they were on 10.1.23\n",
    "(The leading model--Sionic v2--doesn't currently have an API)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c34a2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/scratch/yaarbel'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7662dc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def fetch_embeddings(inputs):\n",
    "    response = requests.post(\n",
    "        'https://api.sionic.ai/v1/embedding',\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "        json={\"inputs\": inputs}\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        return np.array(response.json()['embedding'])\n",
    "    else:\n",
    "        print(\"Error:\", response.content)\n",
    "        return None\n",
    "\n",
    "def calculate_similarities(all_embeddings, sentences, context):\n",
    "    context_embedding = all_embeddings[0]\n",
    "    sentence_embeddings = all_embeddings[1:]\n",
    "    sim_matrix = cosine_similarity([context_embedding], sentence_embeddings)\n",
    "    context_sim_scores = sim_matrix[0]\n",
    "    \n",
    "    # Dynamically set unwanted terms to the current context\n",
    "    unwanted_terms = {context}\n",
    "    print(f\"Unwanted terms: {unwanted_terms}\")  # Debugging\n",
    "    \n",
    "    filtered_sentences = [s for s in sentences if s not in unwanted_terms]\n",
    "    print(f\"Filtered sentences: {filtered_sentences}\")  # Debugging\n",
    "    \n",
    "    filtered_scores = np.array([score for s, score in zip(sentences, context_sim_scores) if s not in unwanted_terms])\n",
    "    \n",
    "    # Normalize the filtered_scores\n",
    "    min_val, max_val = np.min(filtered_scores), np.max(filtered_scores)\n",
    "    normalized_scores = (filtered_scores - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Create DataFrame with both Raw and Normalized Similarity\n",
    "    df = pd.DataFrame({\n",
    "        'Term': filtered_sentences, \n",
    "        'Normalized Similarity': normalized_scores, \n",
    "        'Raw Similarity': filtered_scores\n",
    "    })\n",
    "    \n",
    "    # Sort by Normalized Similarity\n",
    "    df = df.sort_values(by='Normalized Similarity', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Add a Rank column\n",
    "    df['Rank'] = df.index + 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def save_embeddings(model_name, embeddings):\n",
    "    directory = os.path.join(config['output_directory_base'], f\"{model_name}_Embeddings\")\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    filepath = os.path.join(directory, config['embedding_output_file'])\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(embeddings, f)\n",
    "        print(f\"Saved embeddings to {filepath}\")\n",
    "\n",
    "def save_to_csv(model_name, df):\n",
    "    filepath = os.path.join(config['output_directory_base'], config['csv_output_file'])\n",
    "    with open(filepath, 'a') as f:\n",
    "        df['Model'] = model_name  # Add a column for the model name\n",
    "        df.to_csv(f, header=f.tell()==0, index=False)  # Write header only if file is empty\n",
    "\n",
    "\n",
    "def save_to_excel(writer, model_name, df):\n",
    "    df.to_excel(writer, sheet_name=model_name, index=False)\n",
    "\n",
    "def process_model(model_name, model, sentences, context, is_instructor=False):\n",
    "    print (f\"Processing {model_name}\")\n",
    "    df = None  # Initialize df to None\n",
    "    all_embeddings = None  # Initialize all_embeddings to None\n",
    "    try:\n",
    "        print(f\"Processing {model_name}...\")\n",
    "        if is_instructor:\n",
    "            instruction = \"Represent the sentence; Input: \"\n",
    "            all_inputs = [[instruction, context]] + [[instruction, sentence] for sentence in sentences]\n",
    "            all_embeddings = model.encode(all_inputs)\n",
    "        else:\n",
    "            all_inputs = [context] + sentences\n",
    "            if model_name == 'sionic-v1':\n",
    "                all_embeddings = fetch_embeddings(all_inputs)\n",
    "            else:\n",
    "                all_embeddings = model.encode(all_inputs)\n",
    "        df = calculate_similarities(all_embeddings, sentences, context)\n",
    "        if df is not None:  # Only attempt to save if df is not None\n",
    "            save_to_csv(model_name, df)\n",
    "        print(f\"Successfully processed {model_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while processing {model_name}: {e}\")\n",
    "    finally:\n",
    "        try:\n",
    "            if df is not None:  # Only attempt to delete if df is not None\n",
    "                del df\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            if all_embeddings is not None:  # Only attempt to delete if all_embeddings is not None\n",
    "                del all_embeddings\n",
    "        except:\n",
    "            pass\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "   \n",
    "def process_context(context, sentences, models):\n",
    "    config = {\n",
    "        'output_directory_base': '/bighome/yaarbel/temp/',\n",
    "        'csv_output_file': f'All_Model_Results_{context[:20]}.csv',  # Using the first 20 characters of context to keep filename manageable\n",
    "        'embedding_output_file': f'embeddings_{context[:20]}.pkl',   # Similarly, for the embeddings filename\n",
    "    }\n",
    "    for model_name, model, is_instructor in models:\n",
    "        process_model(model_name, model, sentences, context, is_instructor)\n",
    "    print(f\"Processed for context: {context[:50]}...\")  # Printing the first 50 characters of the context for a brief log\n",
    "\n",
    "# Define the set of sentences\n",
    "sentences = [\n",
    "    \"flood\", \"broken water main\", \"heavy rainfall\", \"severe storm\", \"dam failure\", \n",
    "    \"tears of joy\", \"construction near a water body\", \"irrigation canals overflow\",\n",
    "    \"improper drainage\", \"broken levee\", \"burst pipe\", \"monsoon rains\", \"tsunami\",\n",
    "    \"wind\", \"police\", \"fire\"\n",
    "]\n",
    "\n",
    "models= [\n",
    "    ('sionic-v1', None, False),\n",
    "    ('sentence-transformers/sentence-t5-xxl', SentenceTransformer('sentence-transformers/sentence-t5-xxl'), False),\n",
    "    ('thenlper/gte-large', SentenceTransformer('thenlper/gte-large'), False),\n",
    "    ('thenlper/gte-base', SentenceTransformer('thenlper/gte-base'), False),\n",
    "    ('thenlper/gte-small', SentenceTransformer('thenlper/gte-small'), False),\n",
    "    ('hkunlp/instructor-large', INSTRUCTOR('hkunlp/instructor-large'), True),\n",
    "    ('hkunlp/instructor-xl', INSTRUCTOR('hkunlp/instructor-xl'), True),\n",
    "    ('hkunlp/instructor-base', INSTRUCTOR('hkunlp/instructor-base'), True),\n",
    "    ('BAAI/bge-large-en', SentenceTransformer('BAAI/bge-large-en'), False),\n",
    "    ('BAAI/bge-base-en-v1.5', SentenceTransformer('BAAI/bge-base-en-v1.5'), False),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee2963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the contexts\n",
    "contexts = [\n",
    "    \"A flood exception in an insurance policy\",\n",
    "    \"A natural disaster exception in an insurance policy\",\n",
    "    \"A asdf123 exception in an insurance policy\",\n",
    "    '''We do not insure for loss caused directly or indirectly by any of the following. Such loss is excluded regardless of any other cause or event contributing concurrently or in any sequence to the loss. Water Damage, meaning:  Flood, surface water, waves, tidal water, overflow of a body of water, or spray from any of these, whether or not driven by wind.'''\n",
    "]\n",
    "\n",
    "\n",
    "for context in contexts:\n",
    "    process_context(context, sentences, models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
